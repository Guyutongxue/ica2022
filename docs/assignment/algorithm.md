---
# https://github.com/vuejs/vitepress/issues/529
head:
  - - link
    - rel: stylesheet
      href: https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css
  # - - link
  #   - rel: stylesheet
  #     href: https://cdn.bootcdn.net/ajax/libs/KaTeX/0.16.3/katex.min.css
---

# 算法提示

> 本文由李戈班助教王泽钧编写。

## 最直接的人工策略设计

助教话音刚落，就有高手“啪”地站起来了：

“棋盘上的格点，要么黑方下或白方下必输，要么随便下不影响输赢。最简单的想法，你让棋盘上只剩下那些对方一下就输的格点就完了。不过，有些格点不管黑的还是白的，谁下都会输。棋盘长成这种类型，你就这样……”

高手到黑板上开始画棋谱，这些棋谱都体现出了一些特征，根据这些**特征**，有一些应对的**策略**。这些策略，有的很直观，有的很难懂。很快啊，黑板就被画满了。有勤奋的同学就把棋谱抄了下来，写成程序让助教去 PK。面对有备而来的 AI，助教一次都没赢过。

不过后来这些棋谱失传了，江湖上只剩下了“眼”，“打吃”等不知道是什么意思的名词。

## 简单步法搜索

一位同学说，这样设计规则太不“计算机”了，比的是谁下棋能力强。咱们应该让计算机干更多的活。

鲁迅曾经说过，第一层的打不过第五层的。想要起飞就必须懂得这个道理。具体来说，你在做决策的时候必须要考虑到对手的决策，自己根据对手的决策做出的决策，对手根据你对于对手的决策做出的决策所做出的决策……如此套娃。如果两个人都足够聪明，只不过记忆力有差别，你比他能够记住多套一层的结果，那么你就赢了。如果现在是你要下棋，你面对的场面是 $n$，你应该考虑你的对手将如何对你的决定进行反应。假设你可以做出 $m$ 种合法的下子方法，下子方法 $i$ 对应的棋盘场面是 $n_i$，那么你的对手一定会采取在场面 $n_i$ 下对他最有利的走法，假设这种走法对他的收益是 $v_i$。显而易见，你下棋的目的是为了让对手即便使用了最好的策略也得收获最少的好处，因此你在这一步做的决策应该是让棋盘变成对手采用最优解获得的收益最小的那个场面的决策。对手收益最小，你的收益就是最大的，而你下棋的决策就应该是让你获得最大收益的决策。假设对手最小的收益是 $v = \min\{v_1, v_2, ... v_m\}$，那么你的收益就是 $-v$。但你如何获得 $v_i$，你就要想象你是对手，正在根据你的决策，运用同样的决策过程进行他的选择。如此往复循环下去，直到你不想继续了，或者游戏结束了，分出胜负了。显然这是一个递归的过程，以上的方法叫做[负极大值搜索](#ref-minmax)。

不过这样设计有两个问题。其一，搜索空间太大了。这一点我们可以设计剪枝和限制搜索层数来部分解决；其二，根据课上学的，这里的搜索是一个递归的过程，递归要有一个终结的点，也就是搜索树的叶子节点，那叶子节点的值我们怎样评估呢？

助教说：“我要是会，我还要问你？”

## MCTS 搜索

有同学说，我规则不会写，剪枝又不会判，只能在第二层，别人都在第五层，只能退课才能维持的了绩点这样子（可能这个作业发布的时候退课已经截止了）。

不过题还是要做的。我们想一想，比别人层数低的原因是什么，不就是追求了搜索的覆盖面导致不够深入。那么我们为什么不收紧一些搜索的宽度，去追求一些搜索的深度呢？

一个最基础的想法是，从当前局面出发，我们进行 $K$ 个完整的对局，每个对局的每一步的产生，可以是随机的，也可以是根据一定规则的（比如著名的 UCT 算法）。接下来我们要选择我们的动作。假如我们要选择的动作是 $a_i$，那么我们就看看这完成的 $K$ 个对局中有哪些是以 $a_i$ 开始的，这些以 $a_i$ 开始的对局中有哪些是胜利的，这样我们就粗略地估计了 $a_i$ 的胜率。我们选择胜率最高的那个动作即可。

不过这里也有很多技巧，比如说，我们进行的对局数越多越好，但是时间不允许，我们是不是可以每次不把对局下完，用其他的估值方式取代对胜率的计算；或者是，在模拟对局时采用的生成的动作的方法更精细一些，或者……

说到这里，一些同学开始 Google [各种论文](#ref-mcts)，一些同学默默地退出了直播间，掏出草纸，认真地构造剪枝方法、[估值函数](#ref-eval)和棋盘特征。

“菜鸡助教，你等着，你这辈子都打不赢我的 AI。”一个讨厌这个装杯助教的学生如是说。

## 参考资料

<ol>
<li id="ref-minmax">关于<strong>负极大值搜索</strong>，中文资料可以自行百度，英文资料建议参考文章 <i>A Comparative Study of Game Tree Searching Methods</i>。</li>
<li id="ref-mcts">关于 <strong>MCTS 方法</strong>，可以参考文章 <i>Revisiting Monte-Carlo Tree Search on a Normal Form Game: NoGo</i>。</li>
<li id="ref-eval">关于<strong>估值函数</strong>，尚未发现比较经典的方法，可以酌情参考 <i>Artificial Intelligence Algorithm for the Game of NoGo based on Value Evaluation</i> 一文。不过要注意的是，这篇文章的文笔十分捉急，可能会带来诸如误解等阅读障碍，需要格外注意。</li>

</ol>
